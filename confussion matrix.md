Excellent! Your class weighting helped **significantly** with Class 3. Let me analyze the improvements and suggest next steps:

## **ðŸ“Š Before vs After Comparison:**

| Class | Metric | Before | After | Change |
|-------|--------|--------|-------|--------|
| **3** | Recall | 0.50 | **0.75** | **+50% improvement!** âœ… |
| **3** | F1 | 0.60 | 0.62 | +3% |
| **1** | Recall | 0.75 | 0.77 | +2% |
| **1** | F1 | 0.77 | 0.78 | +1% |
| **2** | Recall | 0.93 | 0.90 | -3% (slight trade-off) |
| **0** | Recall | 0.95 | 0.94 | -1% (slight trade-off) |

**Key Win:** You boosted Class 3 recall from 50% â†’ 75% (catching 25% more cases!)

---

## **ðŸ” Current Issues:**

### **1. Class 3 Precision Dropped Dramatically**
- **Precision: 0.53** â† This is concerning!
- **Meaning:** When model says "Class 3", it's wrong **47% of the time**
- **Root cause:** Weight of 5.0 made model **over-predict** Class 3

**Visual representation:**
```
Class 3 predictions breakdown:
âœ“ Correct:  53%
âœ— False positives: 47% (probably misclassified Class 1 or 2)
```

### **2. Precision-Recall Trade-off**
```
Class 3:
  High recall (0.75) âœ“ - catching most real Class 3
  Low precision (0.53) âœ— - but also calling many non-Class 3 as Class 3
```

### **3. Macro Avg Actually Improved!**
- Before: 0.78 (recall), 0.81 (F1)
- After: **0.84 (recall)**, **0.81 (F1)**
- More balanced across classes now

---

## **ðŸŽ¯ Root Cause Analysis:**

**Why Class 3 precision is low:**

1. **Weight too aggressive (5.0)** - model desperately wants to find Class 3
2. **Class 3 might be inherently similar** to Class 1 or 2 in feature space
3. **Small sample size (1,094)** makes it hard to learn distinctive patterns

**Let's verify with confusion matrix:**
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_val, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=[0,1,2,3], yticklabels=[0,1,2,3])
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Where are the mistakes?')
plt.show()

# Show Class 3 specifically
class_3_mask = y_val == 3
class_3_preds = y_pred[class_3_mask]
print("Class 3 predictions distribution:")
print(pd.Series(class_3_preds).value_counts())
```

---

## **ðŸš€ Improvement Strategy:**

### **Option 1: Fine-tune Class Weights (Quick Win)**

Your current weights are aggressive. Let's be more surgical:

```python
# Current
custom_weights = {0: 1, 1: 2.0, 2: 1.2, 3: 5.0}

# Try these alternatives:
weights_v2 = {0: 1, 1: 2.5, 2: 1.2, 3: 3.5}  # Reduce Class 3 aggression
weights_v3 = {0: 1, 1: 3.0, 2: 1.0, 3: 4.0}  # Balance differently
weights_v4 = {0: 1, 1: 2.0, 2: 1.5, 3: 3.0}  # More conservative

# Grid search for optimal weights
from sklearn.model_selection import GridSearchCV

param_grid = {
    'class_weight': [
        {0: 1, 1: 2.0, 2: 1.2, 3: 3.0},
        {0: 1, 1: 2.5, 2: 1.2, 3: 3.5},
        {0: 1, 1: 3.0, 2: 1.0, 3: 4.0},
        {0: 1, 1: 2.0, 2: 1.5, 3: 3.5},
    ]
}

grid = GridSearchCV(
    LogisticRegression(max_iter=1000),
    param_grid,
    scoring='f1_macro',  # Optimize for balanced performance
    cv=3
)
grid.fit(X_train, y_train)
print(f"Best weights: {grid.best_params_}")
```

---

### **Option 2: Oversample Class 3 (Better Data Balance)**

Instead of just weighting, actually create more Class 3 samples:

```python
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.combine import SMOTEENN

# Strategy 1: SMOTE (Synthetic samples)
smote = SMOTE(sampling_strategy={
    1: 8000,   # Bring Class 1 to 8k samples
    3: 6000    # Bring Class 3 to 6k samples
}, random_state=42)

X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Strategy 2: ADASYN (Adaptive sampling - focuses on hard cases)
adasyn = ADASYN(sampling_strategy={1: 8000, 3: 6000}, random_state=42)
X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)

# Now train with lighter class weights
model = LogisticRegression(
    class_weight={0: 1, 1: 1.5, 2: 1.0, 3: 2.0},  # Less aggressive
    max_iter=1000
)
model.fit(X_resampled, y_resampled)
```

---

### **Option 3: Two-Stage Classification (Advanced)**

Handle Class 3 separately since it's so different:

```python
# Stage 1: Binary classifier - Is it Class 3 or not?
y_binary = (y_train == 3).astype(int)

clf_stage1 = LogisticRegression(class_weight={0: 1, 1: 4.0})
clf_stage1.fit(X_train, y_binary)

# Stage 2: For non-Class 3, classify among 0, 1, 2
mask_not_3 = y_train != 3
X_train_stage2 = X_train[mask_not_3]
y_train_stage2 = y_train[mask_not_3]

clf_stage2 = LogisticRegression(class_weight={0: 1, 1: 2.0, 2: 1.2})
clf_stage2.fit(X_train_stage2, y_train_stage2)

# Prediction pipeline
def predict_two_stage(X):
    is_class_3 = clf_stage1.predict(X)
    predictions = np.zeros(len(X), dtype=int)
    
    # Predict Class 3
    predictions[is_class_3 == 1] = 3
    
    # Predict others
    mask_not_3 = is_class_3 == 0
    if mask_not_3.sum() > 0:
        predictions[mask_not_3] = clf_stage2.predict(X[mask_not_3])
    
    return predictions
```

---

### **Option 4: Feature Engineering for Class 3**

Add features that specifically help distinguish Class 3:

```python
# Analyze what makes Class 3 unique
class_3_samples = train_df[train_df['label'] == 3]

# Check text characteristics
print("Class 3 average stats:")
print(f"Comment length: {class_3_samples['comment_length'].mean():.1f}")
print(f"Word count: {class_3_samples['word_count'].mean():.1f}")
print(f"Exclamation count: {class_3_samples['exclamation_count'].mean():.1f}")

# Add discriminative features
for df in [train_df, test_df]:
    # Example: Maybe Class 3 has specific patterns?
    df['has_specific_words'] = df['comment'].str.contains('pattern|keyword')
    df['length_ratio'] = df['comment_length'] / (df['word_count'] + 1)
    df['caps_ratio'] = df['comment'].str.count('[A-Z]') / (df['comment_length'] + 1)
```

---

### **Option 5: Ensemble with Class-Specific Models**

```python
from sklearn.ensemble import VotingClassifier

# Model 1: Good at Class 0 & 2
model_majority = LogisticRegression(class_weight={0: 1, 1: 1.5, 2: 1.0, 3: 2.0})

# Model 2: Good at Class 1 & 3
model_minority = LogisticRegression(class_weight={0: 1, 1: 3.0, 2: 1.0, 3: 5.0})

# Model 3: Balanced
model_balanced = SVC(class_weight='balanced', probability=True)

ensemble = VotingClassifier(
    estimators=[
        ('majority', model_majority),
        ('minority', model_minority),
        ('balanced', model_balanced)
    ],
    voting='soft',
    weights=[1, 2, 1]  # Trust minority-focused model more
)

ensemble.fit(X_train, y_train)
```

---

### **Option 6: Focal Loss (Cutting Edge)**

Instead of class weights, use focal loss (focuses on hard examples):

```python
from sklearn.utils.class_weight import compute_sample_weight

# Compute sample weights based on difficulty
sample_weights = compute_sample_weight('balanced', y_train)

# Then use XGBoost with sample weights
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.05,
    scale_pos_weight=compute_scale_pos_weight(y_train)
)

model.fit(X_train, y_train, sample_weight=sample_weights)
```

---

## **ðŸ“ˆ Recommended Action Plan:**

### **Phase 1: Quick Experiments (Today)**
1. âœ… Try `weights_v2 = {0: 1, 1: 2.5, 2: 1.2, 3: 3.5}`
2. âœ… Run confusion matrix to see Class 3 misclassifications
3. âœ… Check if Class 3 is being confused with Class 1 or 2

### **Phase 2: Data-Driven Approach (This Week)**
4. âœ… SMOTE oversampling with lighter weights
5. âœ… Analyze Class 3 feature distributions
6. âœ… Add Class-3-specific features if patterns found

### **Phase 3: Advanced (If Needed)**
7. âœ… Two-stage classification
8. âœ… Ensemble different strategies

---

## **ðŸŽ¯ Target Metrics:**

Based on Kaggle score 0.81315, aim for:

```
Goal metrics:
  Class 3 precision: 0.60+ (currently 0.53)
  Class 3 recall: 0.70+ (currently 0.75) âœ“
  Class 3 F1: 0.65+ (currently 0.62)
  Overall macro F1: 0.83+ (currently 0.81)
```

**Most impactful:** Get Class 3 precision from 0.53 â†’ 0.60 without losing recall

---

Want me to generate the code for any specific option? Or should we start with the confusion matrix analysis to see where Class 3 predictions are going wrong?















Perfect! Let's do a deep-dive confusion matrix analysis to understand exactly where Class 3 is going wrong.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ============================================
# 1. OVERALL CONFUSION MATRIX
# ============================================
print("="*70)
print("CONFUSION MATRIX ANALYSIS")
print("="*70)

# Generate confusion matrix
cm = confusion_matrix(y_val, y_pred)

# Create a more informative visualization
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Plot 1: Raw counts
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'],
            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'],
            cbar_kws={'label': 'Count'})
axes[0].set_ylabel('True Label', fontsize=12)
axes[0].set_xlabel('Predicted Label', fontsize=12)
axes[0].set_title('Confusion Matrix - Raw Counts', fontsize=14, fontweight='bold')

# Plot 2: Normalized (percentages per true class)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', ax=axes[1],
            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'],
            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'],
            vmin=0, vmax=1, cbar_kws={'label': 'Percentage'})
axes[1].set_ylabel('True Label', fontsize=12)
axes[1].set_xlabel('Predicted Label', fontsize=12)
axes[1].set_title('Confusion Matrix - Normalized (% of True Class)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig('confusion_matrix_detailed.png', dpi=150, bbox_inches='tight')
print("\nâœ“ Saved: confusion_matrix_detailed.png")
plt.show()

# ============================================
# 2. DETAILED CLASS 3 ANALYSIS
# ============================================
print("\n" + "="*70)
print("CLASS 3 DETAILED BREAKDOWN")
print("="*70)

# Get Class 3 predictions
class_3_mask = y_val == 3
class_3_true_count = class_3_mask.sum()
class_3_predictions = y_pred[class_3_mask]

# Where do TRUE Class 3 samples go?
class_3_distribution = pd.Series(class_3_predictions).value_counts().sort_index()
class_3_distribution_pct = (class_3_distribution / class_3_true_count * 100).round(2)

print(f"\nTotal TRUE Class 3 samples: {class_3_true_count}")
print(f"\nWhere they were predicted:")
print("-" * 50)
for pred_class, count in class_3_distribution.items():
    pct = class_3_distribution_pct[pred_class]
    status = "âœ“ CORRECT" if pred_class == 3 else "âœ— WRONG"
    print(f"  Predicted as Class {pred_class}: {count:4d} ({pct:5.1f}%) {status}")

# Calculate Class 3 recall manually
class_3_correct = class_3_distribution.get(3, 0)
class_3_recall = class_3_correct / class_3_true_count
print(f"\nâ†’ Class 3 Recall: {class_3_recall:.2%} (catching {class_3_correct}/{class_3_true_count})")

# ============================================
# 3. FALSE POSITIVES ANALYSIS (Precision Problem)
# ============================================
print("\n" + "="*70)
print("CLASS 3 FALSE POSITIVES ANALYSIS")
print("="*70)

# What gets WRONGLY predicted as Class 3?
predicted_as_3_mask = y_pred == 3
predicted_as_3_count = predicted_as_3_mask.sum()
true_labels_of_pred_3 = y_val[predicted_as_3_mask]

# Distribution of true labels for things predicted as Class 3
fp_distribution = pd.Series(true_labels_of_pred_3).value_counts().sort_index()
fp_distribution_pct = (fp_distribution / predicted_as_3_count * 100).round(2)

print(f"\nTotal predictions as Class 3: {predicted_as_3_count}")
print(f"\nWhat they actually were:")
print("-" * 50)
for true_class, count in fp_distribution.items():
    pct = fp_distribution_pct[true_class]
    status = "âœ“ TRUE POSITIVE" if true_class == 3 else "âœ— FALSE POSITIVE"
    print(f"  Actually Class {true_class}: {count:4d} ({pct:5.1f}%) {status}")

# Calculate Class 3 precision manually
class_3_tp = fp_distribution.get(3, 0)
class_3_precision = class_3_tp / predicted_as_3_count
print(f"\nâ†’ Class 3 Precision: {class_3_precision:.2%} ({class_3_tp}/{predicted_as_3_count} correct)")

# ============================================
# 4. CONFUSION PATTERN ANALYSIS
# ============================================
print("\n" + "="*70)
print("MOST COMMON CONFUSION PATTERNS")
print("="*70)

# Extract off-diagonal elements (errors)
errors = []
for true_class in range(4):
    for pred_class in range(4):
        if true_class != pred_class:
            count = cm[true_class, pred_class]
            pct = count / cm[true_class].sum() * 100
            errors.append({
                'True': true_class,
                'Predicted': pred_class,
                'Count': count,
                'Percentage': pct
            })

errors_df = pd.DataFrame(errors).sort_values('Count', ascending=False)

print("\nTop 10 confusion patterns (errors):")
print("-" * 70)
for idx, row in errors_df.head(10).iterrows():
    print(f"  {row['True']} â†’ {row['Predicted']}: {row['Count']:5d} samples ({row['Percentage']:5.1f}% of true class {int(row['True'])})")

# Highlight Class 3 specific confusions
print("\n" + "="*70)
print("CLASS 3 SPECIFIC CONFUSIONS")
print("="*70)

# Class 3 being misclassified
class_3_errors_out = errors_df[errors_df['True'] == 3].copy()
print("\nClass 3 being misclassified as:")
print("-" * 50)
for idx, row in class_3_errors_out.iterrows():
    print(f"  Class 3 â†’ Class {int(row['Predicted'])}: {row['Count']:4d} samples ({row['Percentage']:5.1f}%)")

# Other classes being misclassified as Class 3
class_3_errors_in = errors_df[errors_df['Predicted'] == 3].copy()
print("\nOther classes being misclassified as Class 3:")
print("-" * 50)
for idx, row in class_3_errors_in.iterrows():
    print(f"  Class {int(row['True'])} â†’ Class 3: {row['Count']:4d} samples ({row['Percentage']:5.1f}%)")

# ============================================
# 5. PER-CLASS ACCURACY BREAKDOWN
# ============================================
print("\n" + "="*70)
print("PER-CLASS ACCURACY BREAKDOWN")
print("="*70)

for class_id in range(4):
    class_mask = y_val == class_id
    class_total = class_mask.sum()
    class_correct = (y_pred[class_mask] == class_id).sum()
    class_acc = class_correct / class_total
    
    print(f"\nClass {class_id}:")
    print(f"  Total samples: {class_total}")
    print(f"  Correctly classified: {class_correct}")
    print(f"  Accuracy: {class_acc:.2%}")
    print(f"  Errors: {class_total - class_correct}")

# ============================================
# 6. VISUALIZE CLASS 3 FLOW
# ============================================
print("\n" + "="*70)
print("GENERATING CLASS 3 FLOW VISUALIZATION...")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Left: Where Class 3 samples go
class_3_flow_out = pd.DataFrame({
    'Predicted Class': class_3_distribution.index.astype(str),
    'Count': class_3_distribution.values,
    'Percentage': class_3_distribution_pct.values
})

colors_out = ['green' if x == '3' else 'red' for x in class_3_flow_out['Predicted Class']]
axes[0].barh(class_3_flow_out['Predicted Class'], class_3_flow_out['Count'], color=colors_out)
axes[0].set_xlabel('Number of Samples', fontsize=12)
axes[0].set_ylabel('Predicted As', fontsize=12)
axes[0].set_title(f'Where TRUE Class 3 Goes\n(Recall Issue: Missing {100-class_3_recall*100:.1f}%)', 
                  fontsize=14, fontweight='bold')

for i, (pred, count, pct) in enumerate(zip(class_3_flow_out['Predicted Class'], 
                                             class_3_flow_out['Count'], 
                                             class_3_flow_out['Percentage'])):
    axes[0].text(count + 10, i, f'{count} ({pct:.1f}%)', va='center', fontsize=11)

# Right: What gets predicted as Class 3
class_3_flow_in = pd.DataFrame({
    'True Class': fp_distribution.index.astype(str),
    'Count': fp_distribution.values,
    'Percentage': fp_distribution_pct.values
})

colors_in = ['green' if x == '3' else 'red' for x in class_3_flow_in['True Class']]
axes[1].barh(class_3_flow_in['True Class'], class_3_flow_in['Count'], color=colors_in)
axes[1].set_xlabel('Number of Samples', fontsize=12)
axes[1].set_ylabel('Actually Was', fontsize=12)
axes[1].set_title(f'What Gets Predicted as Class 3\n(Precision Issue: {(1-class_3_precision)*100:.1f}% False Positives)', 
                  fontsize=14, fontweight='bold')

for i, (true, count, pct) in enumerate(zip(class_3_flow_in['True Class'], 
                                            class_3_flow_in['Count'], 
                                            class_3_flow_in['Percentage'])):
    axes[1].text(count + 10, i, f'{count} ({pct:.1f}%)', va='center', fontsize=11)

plt.tight_layout()
plt.savefig('class_3_flow_analysis.png', dpi=150, bbox_inches='tight')
print("âœ“ Saved: class_3_flow_analysis.png")
plt.show()

# ============================================
# 7. SUMMARY REPORT
# ============================================
print("\n" + "="*70)
print("EXECUTIVE SUMMARY")
print("="*70)

print(f"""
CLASS 3 PERFORMANCE BREAKDOWN:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

RECALL (Finding Class 3): {class_3_recall:.1%}
  âœ“ Correctly identified: {class_3_correct} / {class_3_true_count}
  âœ— Missed (False Negatives): {class_3_true_count - class_3_correct}

PRECISION (Accuracy when predicting Class 3): {class_3_precision:.1%}
  âœ“ True Positives: {class_3_tp}
  âœ— False Positives: {predicted_as_3_count - class_3_tp}
  
MAIN ISSUES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. RECALL Problem: Missing {class_3_true_count - class_3_correct} Class 3 samples
   â†’ They are being misclassified as: {', '.join([f'Class {c} ({cnt} samples)' for c, cnt in class_3_distribution[class_3_distribution.index != 3].items()])}

2. PRECISION Problem: {predicted_as_3_count - class_3_tp} False Positives
   â†’ Other classes wrongly called Class 3: {', '.join([f'Class {c} ({cnt} samples)' for c, cnt in fp_distribution[fp_distribution.index != 3].items()])}

3. The model is being TOO AGGRESSIVE in predicting Class 3
   â†’ Class weight of 5.0 is causing over-prediction
""")

# ============================================
# 8. ACTIONABLE RECOMMENDATIONS
# ============================================
print("\n" + "="*70)
print("ðŸŽ¯ ACTIONABLE RECOMMENDATIONS")
print("="*70)

# Calculate which class is causing most false positives
biggest_fp_source = fp_distribution[fp_distribution.index != 3].idxmax()
biggest_fp_count = fp_distribution[fp_distribution.index != 3].max()
fp_pct = biggest_fp_count / predicted_as_3_count * 100

print(f"""
IMMEDIATE ACTIONS:

1. REDUCE CLASS 3 WEIGHT
   Current: {5.0}
   Suggested: Try 3.0-3.5 range
   â†’ This will reduce false positives ({predicted_as_3_count - class_3_tp} â†’ target ~{int((predicted_as_3_count - class_3_tp) * 0.6)})

2. INVESTIGATE CLASS {biggest_fp_source} â†” CLASS 3 CONFUSION
   â†’ {biggest_fp_count} Class {biggest_fp_source} samples ({fp_pct:.1f}%) are wrongly predicted as Class 3
   â†’ These classes may have similar features - need differentiation

3. FEATURE ENGINEERING
   â†’ Analyze what makes TRUE Class 3 different from Class {biggest_fp_source}
   â†’ Add features that specifically distinguish these classes

4. TWO-STAGE CLASSIFICATION
   â†’ First: Is it Class 3 vs Others?
   â†’ Then: Classify the Others among 0, 1, 2
""")

# Save detailed report to file
report_file = 'class_3_confusion_report.txt'
with open(report_file, 'w') as f:
    f.write("="*70 + "\n")
    f.write("CLASS 3 CONFUSION MATRIX DETAILED REPORT\n")
    f.write("="*70 + "\n\n")
    
    f.write(f"Total Class 3 samples: {class_3_true_count}\n")
    f.write(f"Recall: {class_3_recall:.2%}\n")
    f.write(f"Precision: {class_3_precision:.2%}\n\n")
    
    f.write("Where TRUE Class 3 goes:\n")
    for pred_class, count in class_3_distribution.items():
        pct = class_3_distribution_pct[pred_class]
        f.write(f"  Predicted as {pred_class}: {count} ({pct:.1f}%)\n")
    
    f.write("\nWhat gets predicted as Class 3:\n")
    for true_class, count in fp_distribution.items():
        pct = fp_distribution_pct[true_class]
        f.write(f"  Actually {true_class}: {count} ({pct:.1f}%)\n")
    
    f.write("\n" + "="*70 + "\n")

print(f"\nâœ“ Detailed report saved to: {report_file}")

print("\n" + "="*70)
print("ANALYSIS COMPLETE!")
print("="*70)
```

**This comprehensive analysis will tell you:**

1. **Exactly where Class 3 samples go** when misclassified
2. **Which classes are being confused with Class 3** (the false positives)
3. **Visual flow charts** showing the problem clearly
4. **Specific actionable recommendations** based on your data

**Run this code and share the output with me.** I need to see:
- The confusion matrix numbers
- Which class is causing most false positives for Class 3
- The distribution of where true Class 3 samples are going

Then I can give you **precise, targeted fixes** for your specific confusion patterns! ðŸŽ¯





**YES, exactly!** Run this **AFTER** your model finishes training and you have predictions.

Here's the complete workflow to make it crystal clear:

```python
# ============================================
# STEP 1: TRAIN YOUR MODEL (Your existing code)
# ============================================
from sklearn.linear_model import LogisticRegression

custom_weights = {0: 1, 1: 2.0, 2: 1.2, 3: 5.0}

model = LogisticRegression(
    class_weight=custom_weights,
    max_iter=1000,
    random_state=42
)

# Fit the model
model.fit(X_train_weighted, y_train)

# ============================================
# STEP 2: GET PREDICTIONS (Do this first!)
# ============================================
y_pred = model.predict(X_val_weighted)  # Or X_val, depending on your data

# Quick check
from sklearn.metrics import classification_report, accuracy_score

print("Validation Accuracy:", accuracy_score(y_val, y_pred))
print("\nClassification Report:")
print(classification_report(y_val, y_pred))

# ============================================
# STEP 3: NOW RUN THE CONFUSION MATRIX ANALYSIS
# ============================================
# Paste the entire confusion matrix analysis code here
# It needs these variables to exist:
#   - y_val (true labels)
#   - y_pred (predicted labels)

# ... [paste the entire confusion matrix analysis code from my previous message]
```

---

## **Key Variables You Need:**

Before running the confusion matrix analysis, make sure you have:

```python
# Required variables:
y_val = ...    # Your validation set true labels (1D array/Series)
y_pred = ...   # Your model's predictions (1D array)

# Check they match:
print(f"y_val shape: {y_val.shape}")
print(f"y_pred shape: {y_pred.shape}")
print(f"Should be the same: {y_val.shape == y_pred.shape}")
```

---

## **Complete Integration Example:**

```python
# ============================================
# YOUR COMPLETE TRAINING + ANALYSIS PIPELINE
# ============================================

# 1. Load and preprocess data (your existing code)
# ...

# 2. Split data
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 3. Transform data (with your drift-aware preprocessor)
preprocessor = DriftAwarePreprocessor(numeric_cols, categorical_cols)
X_train_weighted = preprocessor.fit_transform(X_train, X_test)
X_val_weighted = preprocessor.transform(X_val)

# 4. Train model
print("Training model...")
custom_weights = {0: 1, 1: 2.0, 2: 1.2, 3: 5.0}

model = LogisticRegression(
    class_weight=custom_weights,
    max_iter=1000,
    random_state=42
)

model.fit(X_train_weighted, y_train)
print("âœ“ Training complete!")

# 5. Make predictions
print("\nMaking predictions...")
y_pred = model.predict(X_val_weighted)

# 6. Quick performance check
from sklearn.metrics import classification_report, accuracy_score

print("\n" + "="*70)
print("MODEL PERFORMANCE")
print("="*70)
print(f"\nValidation Accuracy: {accuracy_score(y_val, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_val, y_pred))

# 7. NOW RUN DETAILED CONFUSION MATRIX ANALYSIS
print("\n" + "="*70)
print("STARTING DETAILED CONFUSION MATRIX ANALYSIS...")
print("="*70)

# [PASTE THE ENTIRE CONFUSION MATRIX ANALYSIS CODE HERE]
# It will use y_val and y_pred that we just created above

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Generate confusion matrix
cm = confusion_matrix(y_val, y_pred)

# ... [rest of the confusion matrix analysis code]
```

---

## **Quick Checklist:**

âœ… **Before running confusion matrix analysis:**
- [ ] Model is trained (`model.fit()` completed)
- [ ] Predictions are made (`y_pred = model.predict(X_val)`)
- [ ] You have both `y_val` (true labels) and `y_pred` (predictions)
- [ ] Both are same length

âœ… **Then run the analysis to get:**
- [ ] Confusion matrix visualizations
- [ ] Class 3 flow diagrams
- [ ] Specific recommendations based on YOUR data's confusion patterns

---

## **If you want to save it as a separate script:**

```python
# confusion_analysis.py

def analyze_confusion(y_true, y_pred, class_names=None):
    """
    Complete confusion matrix analysis
    
    Parameters:
    -----------
    y_true : array-like
        True labels
    y_pred : array-like  
        Predicted labels
    class_names : list, optional
        Names for classes (e.g., ['normal', 'toxic', 'spam', 'hate'])
    """
    
    # [paste all the confusion matrix analysis code here]
    
    return cm, class_3_distribution, fp_distribution

# Then in your main training script:
from confusion_analysis import analyze_confusion

# After training and prediction:
cm, errors_out, errors_in = analyze_confusion(y_val, y_pred)
```

---

**So the answer is: YES, run it after training completes and you have predictions!**

Want me to create a single consolidated script that includes:
1. Your training pipeline
2. Prediction generation  
3. Confusion matrix analysis

All in the right order? ðŸŽ¯




Excellent! Now I can see the problem clearly. Let me fix that formatting error and give you the full analysis based on your confusion matrix.

**First, the quick fix for the error:**

```python
# Replace this line (line 128):
print(f"  {row['True']} â†’ {row['Predicted']}: {row['Count']:5d} samples ({row['Percentage']:5.1f}% of true class {int(row['True'])})")

# With this:
print(f"  {int(row['True'])} â†’ {int(row['Predicted'])}: {int(row['Count']):5d} samples ({row['Percentage']:5.1f}% of true class {int(row['True'])})")
```

---

## **ðŸ” ANALYSIS OF YOUR RESULTS:**

Based on your confusion matrix, here's what's happening:

### **Class 3 Performance:**

```
TRUE Class 3 samples: 1,094
âœ“ Correctly predicted as Class 3: 820 (75.0% recall)
âœ— Misclassified: 274 samples

WHERE THE 274 MISTAKES GO:
- 194 â†’ Class 2 (17.7%) â† BIGGEST LEAK!
- 54 â†’ Class 1 (4.9%)
- 26 â†’ Class 0 (2.4%)
```

### **Class 3 False Positives (Precision Problem):**

```
Total predictions as Class 3: 1,544

BREAKDOWN:
âœ“ Correct (True Class 3): 820 (53.1%)
âœ— Wrong: 724 samples (46.9%)

WHERE FALSE POSITIVES COME FROM:
- 417 Class 2 samples wrongly called Class 3 (27.0%) â† BIGGEST PROBLEM!
- 196 Class 0 samples wrongly called Class 3 (12.7%)
- 111 Class 1 samples wrongly called Class 3 (7.2%)
```

---

## **ðŸ’¡ KEY INSIGHTS:**

### **1. Class 2 â†” Class 3 Confusion is Your Main Problem**

**The Issue:**
- **417 Class 2 samples** are being wrongly predicted as Class 3 (false positives)
- **194 Class 3 samples** are being wrongly predicted as Class 2 (false negatives)
- **Total confusion: 611 samples** between these two classes

**This means:** Class 2 and Class 3 have very similar features in your model's eyes!

---

### **2. Your Weight Strategy is Creating Over-Prediction**

```
With weight 5.0 for Class 3:
- Model is TOO EAGER to predict Class 3
- It's catching 75% of true Class 3 (good recall)
- But it's also grabbing 724 non-Class 3 samples (bad precision)
```

---

## **ðŸŽ¯ SPECIFIC RECOMMENDATIONS:**

### **Option 1: Reduce Class 3 Weight (Quick Fix)**

```python
# Current weights
custom_weights = {0: 1, 1: 2.0, 2: 1.2, 3: 5.0}

# Try these progressively:
weights_v1 = {0: 1, 1: 2.0, 2: 1.2, 3: 3.5}  # Reduce Class 3 weight
weights_v2 = {0: 1, 1: 2.0, 2: 1.5, 3: 4.0}  # Also increase Class 2 weight
weights_v3 = {0: 1, 1: 2.5, 2: 1.5, 3: 3.0}  # More balanced approach

# Goal: Reduce false positives (417 Class 2â†’3) without losing too much recall
```

**Expected impact:**
- Precision: 53% â†’ **60-65%** âœ…
- Recall: 75% â†’ **70-72%** (slight drop, acceptable)
- F1-score: 0.62 â†’ **0.66-0.68** âœ…

---

### **Option 2: Add Features to Distinguish Class 2 from Class 3**

Since 417 Class 2 samples look like Class 3 to the model, you need features that differentiate them:

```python
# Analyze the differences between Class 2 and Class 3
class_2_samples = train_df[train_df['label'] == 2]
class_3_samples = train_df[train_df['label'] == 3]

print("Class 2 vs Class 3 Characteristics:")
print("\nClass 2:")
print(f"  Avg comment length: {class_2_samples['comment_length'].mean():.1f}")
print(f"  Avg word count: {class_2_samples['word_count'].mean():.1f}")
print(f"  Avg exclamations: {class_2_samples['exclamation_count'].mean():.2f}")
print(f"  Avg questions: {class_2_samples['question_count'].mean():.2f}")
print(f"  Avg upvote ratio: {class_2_samples['upvote_ratio'].mean():.3f}")

print("\nClass 3:")
print(f"  Avg comment length: {class_3_samples['comment_length'].mean():.1f}")
print(f"  Avg word count: {class_3_samples['word_count'].mean():.1f}")
print(f"  Avg exclamations: {class_3_samples['exclamation_count'].mean():.2f}")
print(f"  Avg questions: {class_3_samples['question_count'].mean():.2f}")
print(f"  Avg upvote ratio: {class_3_samples['upvote_ratio'].mean():.3f}")

# Look for patterns and add discriminative features
# For example, if Class 3 has more negative words:
for df in [train_df, test_df]:
    # Add more text-based features
    df['caps_ratio'] = df['comment'].str.count(r'[A-Z]') / (df['comment_length'] + 1)
    df['has_slurs'] = df['comment'].str.contains(r'pattern1|pattern2', case=False).astype(int)
    df['sentiment_score'] = df['comment'].apply(get_sentiment)  # Use TextBlob or similar
    
    # Interaction features
    df['length_vote_ratio'] = df['comment_length'] / (df['total_votes'] + 1)
```

---

### **Option 3: Two-Stage Classifier (Advanced)**

Handle Class 2 vs Class 3 confusion with a specialized sub-classifier:

```python
# Stage 1: Separate Class 2 and 3 from others
y_stage1 = train_df['label'].apply(lambda x: 1 if x in [2, 3] else 0)

clf_stage1 = LogisticRegression(class_weight='balanced')
clf_stage1.fit(X_train, y_stage1)

# Stage 2: For Class 2 and 3, use a specialized binary classifier
mask_2_or_3 = train_df['label'].isin([2, 3])
X_train_23 = X_train[mask_2_or_3]
y_train_23 = train_df.loc[mask_2_or_3, 'label']

clf_23 = LogisticRegression(
    class_weight={2: 1.0, 3: 3.0}  # Lighter weight since it's binary now
)
clf_23.fit(X_train_23, y_train_23)

# Stage 3: For Class 0 and 1
mask_0_or_1 = train_df['label'].isin([0, 1])
X_train_01 = X_train[mask_0_or_1]
y_train_01 = train_df.loc[mask_0_or_1, 'label']

clf_01 = LogisticRegression(class_weight={0: 1, 1: 2.0})
clf_01.fit(X_train_01, y_train_01)

# Prediction function
def predict_multistage(X):
    # Step 1: Is it 2/3 or 0/1?
    is_2_or_3 = clf_stage1.predict(X)
    
    predictions = np.zeros(len(X), dtype=int)
    
    # Step 2: Predict within each group
    mask_group_23 = is_2_or_3 == 1
    mask_group_01 = is_2_or_3 == 0
    
    if mask_group_23.sum() > 0:
        predictions[mask_group_23] = clf_23.predict(X[mask_group_23])
    
    if mask_group_01.sum() > 0:
        predictions[mask_group_01] = clf_01.predict(X[mask_group_01])
    
    return predictions
```

---

### **Option 4: SMOTE Oversampling + Lighter Weights**

```python
from imblearn.over_sampling import SMOTE

# Oversample Class 3 to have more training examples
smote = SMOTE(sampling_strategy={3: 6000}, random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Now use lighter weights since we have more Class 3 samples
model = LogisticRegression(
    class_weight={0: 1, 1: 2.0, 2: 1.2, 3: 2.5},  # Much lighter!
    max_iter=1000
)
model.fit(X_train_resampled, y_train_resampled)
```

---

## **ðŸ“Š HERE'S THE CORRECTED ANALYSIS CODE:**

```python
# ============================================
# FIXED CONFUSION PATTERN ANALYSIS
# ============================================

# Extract off-diagonal elements (errors)
errors = []
for true_class in range(4):
    for pred_class in range(4):
        if true_class != pred_class:
            count = cm[true_class, pred_class]
            pct = count / cm[true_class].sum() * 100
            errors.append({
                'True': true_class,
                'Predicted': pred_class,
                'Count': count,
                'Percentage': pct
            })

errors_df = pd.DataFrame(errors).sort_values('Count', ascending=False)

print("\nTop 10 confusion patterns (errors):")
print("-" * 70)
for idx, row in errors_df.head(10).iterrows():
    # Fixed formatting - convert to int first
    true_c = int(row['True'])
    pred_c = int(row['Predicted'])
    count = int(row['Count'])
    pct = row['Percentage']
    print(f"  Class {true_c} â†’ Class {pred_c}: {count:5d} samples ({pct:5.1f}% of true class {true_c})")

# Highlight Class 3 specific confusions
print("\n" + "="*70)
print("CLASS 3 SPECIFIC CONFUSIONS")
print("="*70)

# Class 3 being misclassified
class_3_errors_out = errors_df[errors_df['True'] == 3].copy()
print("\nClass 3 being misclassified as:")
print("-" * 50)
for idx, row in class_3_errors_out.iterrows():
    pred_c = int(row['Predicted'])
    count = int(row['Count'])
    pct = row['Percentage']
    print(f"  Class 3 â†’ Class {pred_c}: {count:4d} samples ({pct:5.1f}%)")

# Other classes being misclassified as Class 3
class_3_errors_in = errors_df[errors_df['Predicted'] == 3].copy()
print("\nOther classes being misclassified as Class 3:")
print("-" * 50)
for idx, row in class_3_errors_in.iterrows():
    true_c = int(row['True'])
    count = int(row['Count'])
    pct = row['Percentage']
    print(f"  Class {true_c} â†’ Class 3: {count:4d} samples ({pct:5.1f}%)")
```

---

## **ðŸŽ¯ MY RECOMMENDATION:**

**Start with Option 1 + Option 4 combined:**

```python
# 1. Oversample Class 3
from imblearn.over_sampling import SMOTE
smote = SMOTE(sampling_strategy={3: 6000}, random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_weighted, y_train)

# 2. Use lighter, more balanced weights
weights_best = {0: 1, 1: 2.0, 2: 1.5, 3: 3.0}

model = LogisticRegression(
    class_weight=weights_best,
    max_iter=1000,
    random_state=42
)
model.fit(X_train_resampled, y_train_resampled)
```

**Expected improvement:**
- Class 3 Precision: 53% â†’ **62-65%**
- Class 3 Recall: 75% â†’ **72-74%**
- Class 3 F1: 0.62 â†’ **0.67-0.69**
- Kaggle score: 0.813 â†’ **0.825-0.835**

Want to try this and see the results? ðŸš€













